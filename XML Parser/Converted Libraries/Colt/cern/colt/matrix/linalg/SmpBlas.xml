<?xml version="1.0" encoding="UTF-8"?>
<source package="cern.colt.matrix.linalg">
  <import name="cern.colt.matrix.DoubleMatrix1D" />
  <import name="cern.colt.matrix.DoubleMatrix2D" />
  <import name="EDU.oswego.cs.dl.util.concurrent.FJTask" />
  <class name="SmpBlas" startLine="14">
    <implements name="Blas" />
    <javadoc>
      <text>* Parallel implementation of the Basic Linear Algebra System for symmetric multi processing boxes.
 * Currently only a few algorithms are parallelised; the others are fully functional, but run in sequential mode.
 * Parallelised are:
 * <ul>
 * <li>{@link #dgemm dgemm} (matrix-matrix multiplication)</li>
 * <li>{@link #dgemv dgemv} (matrix-vector multiplication)</li>
 * <li>{@link #assign(DoubleMatrix2D,cern.colt.function.DoubleFunction) assign(A,function)} (generalized matrix scaling/transform): Strong speedup only for expensive functions like logarithm, sin, etc.</li>
 * <li>{@link #assign(DoubleMatrix2D,DoubleMatrix2D,cern.colt.function.DoubleDoubleFunction) assign(A,B,function)} (generalized matrix scaling/transform): Strong speedup only for expensive functions like pow etc.</li>
 * </ul>
 * In all cases, no or only marginal speedup is seen for small problem sizes; they are detected and the sequential algorithm is used.
 * <h4>Usage</h4>
 * Call the static method {@link #allocateBlas} at the very beginning of your program, supplying the main parameter for SmpBlas, the number of available CPUs.
 * The method sets the public global variable <tt>SmpBlas.smpBlas</tt> to a blas using a maximum of <tt>CPUs</tt> threads, each concurrently processing matrix blocks with the given sequential blas algorithms.
 * Normally there is no need to call <tt>allocateBlas</tt> more than once.
 * Then use <tt>SmpBlas.smpBlas.someRoutine(...)</tt> to run <tt>someRoutine</tt> in parallel.
 * E.g.
 * <table>
 * <td class="PRE"> 
 * <pre>
 * int cpu_s = 4;
 * SmpBlas.allocateBlas(cpu_s, SeqBlas.seqBlas);
 * ...
 * SmpBlas.smpBlas.dgemm(...)
 * SmpBlas.smpBlas.dgemv(...)
 * </pre>
 * </td>
 * </table>
 * Even if you don't call a blas routine yourself, it often makes sense to allocate a SmpBlas, because other matrix library routines sometimes call the blas.
 * So if you're lucky, you get parallel performance for free.
 * <h4>Notes</h4>
 * <ul>
 * <li>Unfortunately, there is no portable means of automatically detecting the
 * number of CPUs on a JVM, so there is no good way to automate defaults.</li>
 * <li>Only improves performance on boxes with > 1 CPUs and VMs with <b>native threads</b>.</li>
 * <li>Currently only improves performance when working on dense matrix types. On sparse types, performance is likely to degrade (because of the implementation of sub-range views)!</li>
 * <li>Implemented using Doug Lea's fast lightweight task framework ({@link EDU.oswego.cs.dl.util.concurrent}) built upon Java threads, and geared for parallel computation.</li>
 * </ul></text>
      <see>EDU.oswego.cs.dl.util.concurrent.FJTaskRunnerGroup</see>
      <see>EDU.oswego.cs.dl.util.concurrent.FJTask</see>
      <author>wolfgang.hoschek@cern.ch</author>
      <version>0.9, 16/04/2000</version>
    </javadoc>
    <javadoc>
      <text>* The public global parallel blas; initialized via {@link #allocateBlas}.
 * Do not modify this variable via other means (it is public).</text>
    </javadoc>
    <declaration type="Blas" name="smpBlas" />
    <declaration type="Blas" name="seqBlas" />
    <declaration type="Smp" name="smp" />
    <declaration type="int" name="maxThreads" />
    <declaration type="int" name="NN_THRESHOLD" />
    <javadoc>
      <text>* Constructs a blas using a maximum of <tt>maxThreads<tt> threads; each executing the given sequential algos.</text>
    </javadoc>
    <method type="constructor" name="SmpBlas" startLine="76" endLine="81">
      <comment>Smp.smp = new Smp(maxThreads);</comment>
    </method>
    <javadoc>
      <text>* Sets the public global variable <tt>SmpBlas.smpBlas</tt> to a blas using a maximum of <tt>maxThreads</tt> threads, each executing the given sequential algorithm; <tt>maxThreads</tt> is normally the number of CPUs.
 * Call this method at the very beginning of your program. 
 * Normally there is no need to call this method more than once.</text>
      <param>maxThreads the maximum number of threads (= CPUs) to be used</param>
      <param>seqBlas the sequential blas algorithms to be used on concurrently processed matrix blocks.</param>
    </javadoc>
    <method type="void" name="allocateBlas" startLine="89" endLine="100">
      <scope startLine="90" endLine="93">
        <declaration type="SmpBlas" name="s" />
        <comment>no need to change anything?</comment>
      </scope>
      <scope startLine="97" endLine="99" />
    </method>
    <method type="void" name="assign" startLine="101" endLine="110">
      <method type="double" name="apply" startLine="104" endLine="107" />
    </method>
    <method type="void" name="assign" startLine="111" endLine="120">
      <method type="double" name="apply" startLine="114" endLine="117" />
    </method>
    <method type="double" name="dasum" startLine="121" endLine="123" />
    <method type="void" name="daxpy" startLine="124" endLine="126" />
    <method type="void" name="daxpy" startLine="127" endLine="129" />
    <method type="void" name="dcopy" startLine="130" endLine="132" />
    <method type="void" name="dcopy" startLine="133" endLine="135" />
    <method type="double" name="ddot" startLine="136" endLine="138" />
    <method type="void" name="dgemm" startLine="139" endLine="238">
      <scope startLine="169" endLine="172" />
      <scope startLine="173" endLine="176" />
      <declaration type="int" name="m" />
      <declaration type="int" name="n" />
      <declaration type="int" name="p" />
      <declaration type="long" name="flops" />
      <declaration type="int" name="noOfTasks" />
      <declaration type="boolean" name="splitB" />
      <declaration type="int" name="width" />
      <scope startLine="194" endLine="197">
        <comment>parallelization doesn't pay off (too much start up overhead)</comment>
      </scope>
      <declaration type="int" name="span" />
      <declaration type="FJTask[]" name="subTasks" />
      <scope startLine="202" endLine="226">
        <declaration type="int" name="offset" />
        <declaration type="DoubleMatrix2D" name="AA" />
        <scope startLine="207" endLine="212" />
        <scope startLine="213" endLine="218" />
        <method type="void" name="run" startLine="221" endLine="224" />
        <comment>last span may be a bit larger</comment>
        <comment>split B along columns into blocks</comment>
        <comment>split A along rows into blocks</comment>
        <comment>System.out.println("Hello "+offset);</comment>
      </scope>
      <scope startLine="229" endLine="237">
        <method type="void" name="run" startLine="232" endLine="234" />
      </scope>
      <scope startLine="237" endLine="237" />
      <comment>determine how to split and parallelize best into blocks
if more B.columns than tasks --> split B.columns, as follows:

xx|xx|xxx B
xx|xx|xxx
xx|xx|xxx
A
xxx     xx|xx|xxx C
xxx		xx|xx|xxx
xxx		xx|xx|xxx
xxx		xx|xx|xxx
xxx		xx|xx|xxx

if less B.columns than tasks --> split A.rows, as follows:

xxxxxxx B
xxxxxxx
xxxxxxx
A
xxx     xxxxxxx C
xxx     xxxxxxx
---     -------
xxx     xxxxxxx
xxx     xxxxxxx
---     -------
xxx     xxxxxxx</comment>
      <comment>each thread should process at least 30000 flops</comment>
      <comment>set up concurrent tasks</comment>
      <comment>run tasks and wait for completion</comment>
    </method>
    <method type="void" name="dgemv" startLine="239" endLine="301">
      <scope startLine="256" endLine="259" />
      <declaration type="int" name="m" />
      <declaration type="int" name="n" />
      <declaration type="long" name="flops" />
      <declaration type="int" name="noOfTasks" />
      <declaration type="int" name="width" />
      <scope startLine="267" endLine="270">
        <comment>parallelization doesn't pay off (too much start up overhead)</comment>
      </scope>
      <declaration type="int" name="span" />
      <declaration type="FJTask[]" name="subTasks" />
      <scope startLine="275" endLine="289">
        <declaration type="int" name="offset" />
        <declaration type="DoubleMatrix2D" name="AA" />
        <declaration type="DoubleMatrix1D" name="yy" />
        <method type="void" name="run" startLine="284" endLine="287" />
        <comment>last span may be a bit larger</comment>
        <comment>split A along rows into blocks</comment>
        <comment>System.out.println("Hello "+offset);</comment>
      </scope>
      <scope startLine="292" endLine="300">
        <method type="void" name="run" startLine="295" endLine="297" />
      </scope>
      <scope startLine="300" endLine="300" />
      <comment>split A, as follows:

x x
x
x
A
xxx     x y
xxx     x
---     -
xxx     x
xxx     x
---     -
xxx     x</comment>
      <comment>each thread should process at least 30000 flops</comment>
      <comment>set up concurrent tasks</comment>
      <comment>run tasks and wait for completion</comment>
    </method>
    <method type="void" name="dger" startLine="302" endLine="304" />
    <method type="double" name="dnrm2" startLine="305" endLine="307" />
    <method type="void" name="drot" startLine="308" endLine="310" />
    <method type="void" name="drotg" startLine="311" endLine="313" />
    <method type="void" name="dscal" startLine="314" endLine="316" />
    <method type="void" name="dscal" startLine="317" endLine="319" />
    <method type="void" name="dswap" startLine="320" endLine="322" />
    <method type="void" name="dswap" startLine="323" endLine="325" />
    <method type="void" name="dsymv" startLine="326" endLine="328" />
    <method type="void" name="dtrmv" startLine="329" endLine="331" />
    <method type="int" name="idamax" startLine="332" endLine="334" />
    <method type="double[]" name="run" startLine="335" endLine="351">
      <declaration type="DoubleMatrix2D[][]" name="blocks" />
      <declaration type="int" name="b" />
      <declaration type="double[]" name="results" />
      <scope startLine="342" endLine="346">
        <declaration type="double" name="result" />
        <comment>too small --> sequential</comment>
      </scope>
      <scope startLine="347" endLine="349">
        <comment>parallel</comment>
      </scope>
      <comment>blocks = this.smp.splitStridedNN(A, B, NN_THRESHOLD, A.rows()*A.columns());</comment>
    </method>
    <method type="double[]" name="run" startLine="352" endLine="368">
      <declaration type="DoubleMatrix2D[]" name="blocks" />
      <declaration type="int" name="b" />
      <declaration type="double[]" name="results" />
      <scope startLine="359" endLine="363">
        <declaration type="double" name="result" />
        <comment>too small -> sequential</comment>
      </scope>
      <scope startLine="364" endLine="366">
        <comment>parallel</comment>
      </scope>
      <comment>blocks = this.smp.splitStridedNN(A, NN_THRESHOLD, A.rows()*A.columns());</comment>
    </method>
    <javadoc>
      <text>* Prints various snapshot statistics to System.out; Simply delegates to {@link EDU.oswego.cs.dl.util.concurrent.FJTaskRunnerGroup#stats}.</text>
    </javadoc>
    <method type="void" name="stats" startLine="372" endLine="374" />
    <method type="double" name="xsum" startLine="375" endLine="387">
      <declaration type="double[]" name="sums" />
      <method type="double" name="apply" startLine="378" endLine="380" />
      <declaration type="double" name="sum" />
    </method>
    <comment>blocks are operated on in parallel; for each block this seq algo is used.</comment>
  </class>
</source>
