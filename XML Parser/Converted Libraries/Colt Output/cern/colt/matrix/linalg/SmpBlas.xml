<?xml version="1.0" encoding="UTF-8"?>
<class name="SmpBlas">
  <javadoc>
    <text>Parallel implementation of the Basic Linear Algebra System for symmetric multi processing boxes.
 * Currently only a few algorithms are parallelised; the others are fully functional, but run in sequential mode.
 * Parallelised are:
 * &lt;ul&gt;
 * &lt;li&gt;{@link #dgemm dgemm} (matrix-matrix multiplication)&lt;/li&gt;
 * &lt;li&gt;{@link #dgemv dgemv} (matrix-vector multiplication)&lt;/li&gt;
 * &lt;li&gt;{@link #assign(DoubleMatrix2D,cern.colt.function.DoubleFunction) assign(A,function)} (generalized matrix scaling/transform): Strong speedup only for expensive functions like logarithm, sin, etc.&lt;/li&gt;
 * &lt;li&gt;{@link #assign(DoubleMatrix2D,DoubleMatrix2D,cern.colt.function.DoubleDoubleFunction) assign(A,B,function)} (generalized matrix scaling/transform): Strong speedup only for expensive functions like pow etc.&lt;/li&gt;
 * &lt;/ul&gt;
 * In all cases, no or only marginal speedup is seen for small problem sizes; they are detected and the sequential algorithm is used.
 * &lt;h4&gt;Usage&lt;/h4&gt;
 * Call the static method {@link #allocateBlas} at the very beginning of your program, supplying the main parameter for SmpBlas, the number of available CPUs.
 * The method sets the public global variable &lt;tt&gt;SmpBlas.smpBlas&lt;/tt&gt; to a blas using a maximum of &lt;tt&gt;CPUs&lt;/tt&gt; threads, each concurrently processing matrix blocks with the given sequential blas algorithms.
 * Normally there is no need to call &lt;tt&gt;allocateBlas&lt;/tt&gt; more than once.
 * Then use &lt;tt&gt;SmpBlas.smpBlas.someRoutine(...)&lt;/tt&gt; to run &lt;tt&gt;someRoutine&lt;/tt&gt; in parallel.
 * E.g.
 * &lt;table&gt;
 * &lt;td class="PRE"&gt; 
 * &lt;pre&gt;
 * int cpu_s = 4;
 * SmpBlas.allocateBlas(cpu_s, SeqBlas.seqBlas);
 * ...
 * SmpBlas.smpBlas.dgemm(...)
 * SmpBlas.smpBlas.dgemv(...)
 * &lt;/pre&gt;
 * &lt;/td&gt;
 * &lt;/table&gt;
 * Even if you don't call a blas routine yourself, it often makes sense to allocate a SmpBlas, because other matrix library routines sometimes call the blas.
 * So if you're lucky, you get parallel performance for free.
 * &lt;h4&gt;Notes&lt;/h4&gt;
 * &lt;ul&gt;
 * &lt;li&gt;Unfortunately, there is no portable means of automatically detecting the
 * number of CPUs on a JVM, so there is no good way to automate defaults.&lt;/li&gt;
 * &lt;li&gt;Only improves performance on boxes with &gt; 1 CPUs and VMs with &lt;b&gt;native threads&lt;/b&gt;.&lt;/li&gt;
 * &lt;li&gt;Currently only improves performance when working on dense matrix types. On sparse types, performance is likely to degrade (because of the implementation of sub-range views)!&lt;/li&gt;
 * &lt;li&gt;Implemented using Doug Lea's fast lightweight task framework ({@link EDU.oswego.cs.dl.util.concurrent}) built upon Java threads, and geared for parallel computation.&lt;/li&gt;
 * &lt;/ul&gt;</text>
    <see>EDU.oswego.cs.dl.util.concurrent.FJTaskRunnerGroup</see>
    <see>EDU.oswego.cs.dl.util.concurrent.FJTask</see>
    <author>wolfgang.hoschek@cern.ch</author>
    <version>0.9, 16/04/2000</version>
  </javadoc>
  <javadoc>
    <text>The public global parallel blas; initialized via {@link #allocateBlas}.
 * Do not modify this variable via other means (it is public).</text>
  </javadoc>
  <declaration type="Blas" name="smpBlas" />
  <declaration type="Blas" name="seqBlas" />
  <declaration type="Smp" name="smp" />
  <declaration type="int" name="maxThreads" />
  <declaration type="int" name="NN_THRESHOLD" />
  <javadoc>
    <text>Constructs a blas using a maximum of &lt;tt&gt;maxThreads&lt;tt&gt; threads; each executing the given sequential algos.</text>
  </javadoc>
  <method type="constructor" name="SmpBlas">
    <comment>Smp.smp = new Smp(maxThreads);</comment>
  </method>
  <javadoc>
    <text>Sets the public global variable &lt;tt&gt;SmpBlas.smpBlas&lt;/tt&gt; to a blas using a maximum of &lt;tt&gt;maxThreads&lt;/tt&gt; threads, each executing the given sequential algorithm; &lt;tt&gt;maxThreads&lt;/tt&gt; is normally the number of CPUs.
 * Call this method at the very beginning of your program. 
 * Normally there is no need to call this method more than once.</text>
    <param>maxThreads the maximum number of threads (= CPUs) to be used</param>
    <param>seqBlas the sequential blas algorithms to be used on concurrently processed matrix blocks.</param>
  </javadoc>
  <method type="void" name="allocateBlas">
    <scope>
      <declaration type="SmpBlas" name="s" />
    </scope>
    <scope />
    <comment>no need to change anything?</comment>
  </method>
  <method type="void" name="assign">
    <method type="double" name="apply" />
  </method>
  <method type="void" name="assign">
    <method type="double" name="apply" />
  </method>
  <method type="double" name="dasum" />
  <method type="void" name="daxpy" />
  <method type="void" name="daxpy" />
  <method type="void" name="dcopy" />
  <method type="void" name="dcopy" />
  <method type="double" name="ddot" />
  <method type="void" name="dgemm">
    <scope />
    <scope />
    <declaration type="int" name="m" />
    <declaration type="int" name="n" />
    <declaration type="int" name="p" />
    <declaration type="long" name="flops" />
    <declaration type="int" name="noOfTasks" />
    <declaration type="boolean" name="splitB" />
    <declaration type="int" name="width" />
    <scope />
    <declaration type="int" name="span" />
    <declaration type="FJTask[]" name="subTasks" />
    <scope>
      <declaration type="int" name="offset" />
      <declaration type="DoubleMatrix2D" name="AA" />
      <scope />
      <scope />
      <method type="void" name="run" />
    </scope>
    <scope>
      <method type="void" name="run" />
    </scope>
    <scope />
    <comment>determine how to split and parallelize best into blocks
if more B.columns than tasks --&gt; split B.columns, as follows:

xx|xx|xxx B
xx|xx|xxx
xx|xx|xxx
A
xxx     xx|xx|xxx C
xxx		xx|xx|xxx
xxx		xx|xx|xxx
xxx		xx|xx|xxx
xxx		xx|xx|xxx

if less B.columns than tasks --&gt; split A.rows, as follows:

xxxxxxx B
xxxxxxx
xxxxxxx
A
xxx     xxxxxxx C
xxx     xxxxxxx
---     -------
xxx     xxxxxxx
xxx     xxxxxxx
---     -------
xxx     xxxxxxx</comment>
    <comment>each thread should process at least 30000 flops</comment>
    <comment>parallelization doesn't pay off (too much start up overhead)</comment>
    <comment>set up concurrent tasks</comment>
    <comment>last span may be a bit larger</comment>
    <comment>split B along columns into blocks</comment>
    <comment>split A along rows into blocks</comment>
    <comment>System.out.println("Hello "+offset);</comment>
    <comment>run tasks and wait for completion</comment>
  </method>
  <method type="void" name="dgemv">
    <scope />
    <declaration type="int" name="m" />
    <declaration type="int" name="n" />
    <declaration type="long" name="flops" />
    <declaration type="int" name="noOfTasks" />
    <declaration type="int" name="width" />
    <scope />
    <declaration type="int" name="span" />
    <declaration type="FJTask[]" name="subTasks" />
    <scope>
      <declaration type="int" name="offset" />
      <declaration type="DoubleMatrix2D" name="AA" />
      <declaration type="DoubleMatrix1D" name="yy" />
      <method type="void" name="run" />
    </scope>
    <scope>
      <method type="void" name="run" />
    </scope>
    <scope />
    <comment>split A, as follows:

x x
x
x
A
xxx     x y
xxx     x
---     -
xxx     x
xxx     x
---     -
xxx     x</comment>
    <comment>each thread should process at least 30000 flops</comment>
    <comment>parallelization doesn't pay off (too much start up overhead)</comment>
    <comment>set up concurrent tasks</comment>
    <comment>last span may be a bit larger</comment>
    <comment>split A along rows into blocks</comment>
    <comment>System.out.println("Hello "+offset);</comment>
    <comment>run tasks and wait for completion</comment>
  </method>
  <method type="void" name="dger" />
  <method type="double" name="dnrm2" />
  <method type="void" name="drot" />
  <method type="void" name="drotg" />
  <method type="void" name="dscal" />
  <method type="void" name="dscal" />
  <method type="void" name="dswap" />
  <method type="void" name="dswap" />
  <method type="void" name="dsymv" />
  <method type="void" name="dtrmv" />
  <method type="int" name="idamax" />
  <method type="double[]" name="run">
    <declaration type="DoubleMatrix2D[][]" name="blocks" />
    <declaration type="int" name="b" />
    <declaration type="double[]" name="results" />
    <scope>
      <declaration type="double" name="result" />
    </scope>
    <scope />
    <comment>blocks = this.smp.splitStridedNN(A, B, NN_THRESHOLD, A.rows()*A.columns());</comment>
    <comment>too small --&gt; sequential</comment>
    <comment>parallel</comment>
  </method>
  <method type="double[]" name="run">
    <declaration type="DoubleMatrix2D[]" name="blocks" />
    <declaration type="int" name="b" />
    <declaration type="double[]" name="results" />
    <scope>
      <declaration type="double" name="result" />
    </scope>
    <scope />
    <comment>blocks = this.smp.splitStridedNN(A, NN_THRESHOLD, A.rows()*A.columns());</comment>
    <comment>too small -&gt; sequential</comment>
    <comment>parallel</comment>
  </method>
  <javadoc>
    <text>Prints various snapshot statistics to System.out; Simply delegates to {@link EDU.oswego.cs.dl.util.concurrent.FJTaskRunnerGroup#stats}.</text>
  </javadoc>
  <method type="void" name="stats" />
  <method type="double" name="xsum">
    <declaration type="double[]" name="sums" />
    <method type="double" name="apply" />
    <declaration type="double" name="sum" />
  </method>
  <comment>Copyright ï¿½ 1999 CERN - European Organization for Nuclear Research.
Permission to use, copy, modify, distribute and sell this software and its documentation for any purpose
is hereby granted without fee, provided that the above copyright notice appear in all copies and
that both that copyright notice and this permission notice appear in supporting documentation.
CERN makes no representations about the suitability of this software for any purpose.
It is provided "as is" without expressed or implied warranty.</comment>
  <comment>blocks are operated on in parallel; for each block this seq algo is used.</comment>
</class>
