package sanlp.nlp.runnables;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map.Entry;
import java.util.Properties;

import sanlp.nlp.core.NGramParser;
import sanlp.nlp.core.Stemmer;
import sanlp.nlp.core.TFIDF;
import sanlp.nlp.output.PMIWriter;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.stats.Counter;

public class BiGramPMI {

	private static int nGramSize = 2;

	// private static final String customStopWordList =
	// ".;,;?;(;);- >;>;<;-;a;an;and;are;as;at;be;but;by;for;if;in;into;is;it;no;not;of;on;or;such;that;the;their;then;there;these;they;this;to;was;will;with;i;'m;";

	/**
	 * Read in a text file (assume UTF-8) into a string using the absolute path given.
	 */
	private static String readTextFile(String path) throws FileNotFoundException, IOException {
		int lines = 0;
		String everything = "";
		try (BufferedReader br = new BufferedReader(new FileReader(path))) {
			StringBuilder sb = new StringBuilder();
			String line = br.readLine();

			while (line != null && lines++ < 25) {
				sb.append(line);
				sb.append(System.lineSeparator());
				line = br.readLine();
			}
			everything = sb.toString();
		}
		return everything;
	}

	/**
	 * Entry point to be used for testing.
	 */
	public static void main(String... args) throws FileNotFoundException, IOException {
		HashMap<String, String> docs = new HashMap<String, String>();

		// Example of passing multiple documents (two) to the 'parser'.
		String file1 = readTextFile("/Users/danhowden/2014-sas-nlp/private/compiled.txt");
		docs.put("SO - general", file1);
		docs.put("Comment - domain",
				"The open source unhandled exception will be serious with complex data in the java runtime");
		
		//You must specify a length for the n-grams.
		calculate(docs, 2);
	}

	/**
	 * This method requires two documents (corpuses... corpi?), a GENERAL document and a DOMAIN document.
	 * It will then parse the documents to produce PMI values for all bi-grams, n-gram AND single word TF-IDF
	 * calculations, as well as calculating the domain specificity of all domain n-grams and single words.
	 */
	public static void calculate(HashMap<String, String> docs, int nGramLength) {
		Properties props = new Properties();
		props.put("tokenize.options",
				"normalizeParentheses=false,americanize=false,untokenizable=firstDelete");
		props.put("annotators", "tokenize, ssplit, pos, lemma, parse");
		// /props.setProperty("customAnnotatorClass.stopword",
		// "sanlp.nlp.core.StopwordAnnotator");
		// props.setProperty(StopwordAnnotator.STOPWORDS_LIST,
		// customStopWordList);

		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
		
		// Used to parse the n-grams of given length
		NGramParser biGramExtractor = new NGramParser(nGramLength);
		
		// Records all  words so that the n-gram parser operates on these words (sequentially).
		List<String> wordList = new ArrayList<String>();
		
		// Counts the number of words that occur based on their frequency in EACH document.
		// For example: {"doc1" : {"word1", "word2"}, "doc2" : {"word9", "word2"}}
		HashMap<String, ClassicCounter<String>> wordFrequencies = new HashMap<String, ClassicCounter<String>>();
		
		// Counts the total number of words in BOTH documents.
		Counter<String> wordsOverall = new ClassicCounter<String>();
		
		// Counts the total number of words in the GENERAL corpus
		Counter<String> wordsGeneral = new ClassicCounter<String>();
		
		// Counts the total number of words in the DOMAIN-specific corpus.
		Counter<String> wordsDomain = new ClassicCounter<String>();
		
		// Records the TF-IDF of single words for EACH document.
		// For example: {"doc1" : {"word1" : 4.23}, "doc2" : {"word1" : 3.324}}
		HashMap<String, HashMap<String, Float>> tfidfSingleWords = new HashMap<String, HashMap<String, Float>>();
		
		// Records the TF-IDF of n-grams (size dictated by nGramLength parameter) for EACH document.
		// For example: {"doc1" : {"this part" : 4.23}, "doc2" : {"this part" : 3.324}}
		HashMap<String, HashMap<String, Float>> tfidfNGrams = new HashMap<String, HashMap<String, Float>>();
		
		// Records n-grams against the document they occurred in.
		HashMap<String, NGramParser> ngramsPerDoc = new HashMap<String, NGramParser>();

		for (Entry<String, String> x : docs.entrySet()) {
			// Parse each document (should only be 2 of them)
			Annotation document = new Annotation(x.getValue().toLowerCase());
			pipeline.annotate(document);

			List<String> wordListForThisDoc = new ArrayList<String>();

			for (CoreLabel token : document.get(TokensAnnotation.class)) {
				
				// The stemmer is a little bit complicated to use, could probably be made cleaner!
				Stemmer stemmer = new Stemmer();
				stemmer.add(token.get(TextAnnotation.class).toLowerCase().toCharArray(), token.get(TextAnnotation.class).length());
				stemmer.stem();
				String word = stemmer.toString();

				// Record this word occurrence for later use.
				wordList.add(word);
				wordListForThisDoc.add(word);
				wordsOverall.incrementCount(word);

				// Keep track of which words appear in which corpus.
				if (x.getKey().contains("general"))
					wordsGeneral.incrementCount(word);
				else
					wordsDomain.incrementCount(word);

				// Record the occurrence of this word AGAINST the document it occurred in.
				// Create key-value if it doesn't exist.
				if (!wordFrequencies.containsKey(x.getKey())) {
					wordFrequencies.put(x.getKey(), new ClassicCounter<String>());
				}

				wordFrequencies.get(x.getKey()).incrementCount(word);

				// Create key-value if it doesn't exist.
				if (!ngramsPerDoc.containsKey(x.getKey())) {
					ngramsPerDoc.put(x.getKey(), new NGramParser(nGramSize));
				}
			}

			// Calculate n-grams for this document based on the words stored.
			ngramsPerDoc.get(x.getKey()).parse(wordListForThisDoc, false, false);
		}

		for (Entry<String, String> x : docs.entrySet()) {

			for (Entry<String, Integer> w : ngramsPerDoc.get(x.getKey()).getNGramMap().entrySet()) {
				String word = w.getKey();

				int docsWithTerm = 0;

				for (Entry<String, NGramParser> i : ngramsPerDoc.entrySet()) {
					if (i.getValue().getNGramMap().containsKey(word))
						docsWithTerm++;
				}

				TFIDF t = new TFIDF(ngramsPerDoc.get(x.getKey()).getNGramMap()
						.get(word), ngramsPerDoc.get(x.getKey())
						.getNgramCount(), docs.size(), docsWithTerm);
				Float tdidf = t.getValue();

				if (!tfidfNGrams.containsKey(x.getKey()))
					tfidfNGrams.put(x.getKey(), new HashMap<String, Float>());

				tfidfNGrams.get(x.getKey()).put(word, tdidf);
			}
		}

		biGramExtractor.parse(wordList, false, false);

		for (Entry<String, String> x : docs.entrySet()) {
			for (Entry<String, Double> w : wordFrequencies.get(x.getKey()).entrySet()) {
				String word = w.getKey();

				int docsWithTerm = 0;

				for (Entry<String, ClassicCounter<String>> i : wordFrequencies.entrySet()) {
					if (i.getValue().getCount(word) > 0)
						docsWithTerm++;
				}

				TFIDF t = new TFIDF(wordFrequencies.get(x.getKey()).getCount(word), 
						wordFrequencies.get(x.getKey()).totalCount(), 
						docs.size(), 
						docsWithTerm);
				Float tdidf = t.getValue();
				
				if (!tfidfSingleWords.containsKey(x.getKey()))
					tfidfSingleWords.put(x.getKey(), new HashMap<String, Float>());

				tfidfSingleWords.get(x.getKey()).put(word, tdidf);
			}
		}

		// Write to the disk.
		PMIWriter pmiWriter = new PMIWriter(
				biGramExtractor, 
				wordsOverall,
				tfidfSingleWords, 
				tfidfNGrams, 
				wordsGeneral, 
				wordsDomain,
				ngramsPerDoc);

		pmiWriter.generate();
	}
}
